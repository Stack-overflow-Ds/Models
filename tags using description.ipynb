{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DataScience.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9MToH6i_OyP"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujMq3Iv6_12A"
      },
      "source": [
        "!pip install -q kaggle\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u193IDVm_-UX",
        "outputId": "5a8b96f5-9248-4c73-8512-e267101ae3b6"
      },
      "source": [
        "!kaggle datasets download -d stackoverflow/stacksample"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading stacksample.zip to /content\n",
            "100% 1.10G/1.11G [00:18<00:00, 39.1MB/s]\n",
            "100% 1.11G/1.11G [00:18<00:00, 64.3MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvJu7J7lACSS",
        "outputId": "b1179fea-9f9d-4073-d7de-79c8f9582fe8"
      },
      "source": [
        "!unzip stacksample.zip"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  stacksample.zip\n",
            "  inflating: Answers.csv             \n",
            "  inflating: Questions.csv           \n",
            "  inflating: Tags.csv                \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WS5LDhCYbqeW",
        "outputId": "5ab99b3d-0598-4668-b98c-4c7eb493b364"
      },
      "source": [
        "!pip install scikit-multilearn"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scikit-multilearn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/1f/e6ff649c72a1cdf2c7a1d31eb21705110ce1c5d3e7e26b2cc300e1637272/scikit_multilearn-0.2.0-py3-none-any.whl (89kB)\n",
            "\r\u001b[K     |███▊                            | 10kB 9.5MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 20kB 13.6MB/s eta 0:00:01\r\u001b[K     |███████████                     | 30kB 12.7MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 40kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 51kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 61kB 6.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 71kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 81kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 92kB 4.3MB/s \n",
            "\u001b[?25hInstalling collected packages: scikit-multilearn\n",
            "Successfully installed scikit-multilearn-0.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ytV8sfsAZH3"
      },
      "source": [
        "# IMPORTS\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from datetime import datetime\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import f1_score,precision_score,recall_score\n",
        "from sklearn import svm\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from skmultilearn.adapt import mlknn\n",
        "from skmultilearn.problem_transform import ClassifierChain\n",
        "from skmultilearn.problem_transform import BinaryRelevance\n",
        "from skmultilearn.problem_transform import LabelPowerset\n",
        "from sklearn.naive_bayes import GaussianNB\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKU9wf4zEZ7-",
        "outputId": "9774bdeb-ef06-4401-b3b8-4b213a25cee1"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fxw_BOxkA-72"
      },
      "source": [
        "questions = pd.read_csv('Questions.csv',encoding=\"ISO-8859-1\")\n",
        "tags = pd.read_csv('Tags.csv', encoding=\"ISO-8859-1\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZs5_ztOBKa2",
        "outputId": "26bac32c-4fa1-4b83-ee91-d00af94f8fea"
      },
      "source": [
        "#Question Structure\n",
        "idx = 8965\n",
        "Id = questions.Id[idx]\n",
        "title = questions.Title[idx]\n",
        "body = questions.Body[idx]\n",
        "score = questions.Score[idx]\n",
        "print(\"ID :\",Id)\n",
        "print(\"Title :\", title)\n",
        "print(\"Body :\", body)\n",
        "curr_tags = tags[tags.Id == Id]['Tag']\n",
        "tags_str = \", \".join(curr_tags.tolist())\n",
        "print(\"Score :\",score)\n",
        "print(\"Tags :\", tags_str)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ID : 584870\n",
            "Title : Why âNo database selectedâ SQLException?\n",
            "Body : <p>why this program is not executing when it goes in to the do while loop second time and why it is giving the exception \"Exception java.sql.SQLException: [MySQL][ODBC 5.1 Driver][mysqld-5.0.51a-community-nt]No database selected\"</p>\n",
            "\n",
            "<pre><code>//import java.io.InputStream;\n",
            "import java.sql.Connection;\n",
            "import java.sql.DriverManager;\n",
            "import java.sql.ResultSet;\n",
            "import java.sql.SQLException;\n",
            "import java.sql.Statement;\n",
            "import java.util.Scanner;\n",
            "import java.util.Vector;\n",
            "\n",
            "public class DataBase {\n",
            "\n",
            "    public void LoadDriver() {\n",
            "\n",
            "        // Load the JDBC-ODBC bridge driver\n",
            "        try {\n",
            "            Class.forName(\"sun.jdbc.odbc.JdbcOdbcDriver\");\n",
            "        } catch (ClassNotFoundException ee) {\n",
            "            ee.printStackTrace();\n",
            "        }\n",
            "    }\n",
            "\n",
            "    // 2.open a data source name by means of the jdbcodbcdriver.\n",
            "\n",
            "    static void connect() throws SQLException {\n",
            "\n",
            "        // Connect to the database\n",
            "        Connection con = DriverManager.getConnection(\"jdbc:odbc:MySQL\", \"root\", \"admin\");\n",
            "        Statement stmt = con.createStatement();\n",
            "        // Shut off autocommit\n",
            "        con.setAutoCommit(false);\n",
            "\n",
            "\n",
            "        System.out.println(\"1.Insert 2.Delete 3.Update 4.Select\");\n",
            "        Scanner s = new Scanner(System.in);\n",
            "        int x;\n",
            "        x = s.nextInt();\n",
            "\n",
            "        String query; // SQL select string\n",
            "        ResultSet rs; // SQL query results\n",
            "        boolean more; // \"more rows found\" switch\n",
            "        String v1, v2; // Temporary storage results\n",
            "\n",
            "        Vector&lt;Object&gt; results = new Vector&lt;Object&gt;(10);\n",
            "\n",
            "\n",
            "        if (x == 1) {\n",
            "\n",
            "            try {\n",
            "                stmt.executeUpdate(\"INSERT INTO employee( emp_id,emp_name ) VALUES ( '122','shiva' ) \");\n",
            "            } catch(Exception e){System.out.println(\"Exception \" +e);e.printStackTrace();}\n",
            "        }\n",
            "\n",
            "        if (x == 2) {\n",
            "\n",
            "            try {\n",
            "                stmt.executeUpdate(\"DELETE from employee where emp_id='102' \");\n",
            "            }catch(Exception e){System.out.println(\"Exception \"+e);e.printStackTrace();} \n",
            "        }\n",
            "\n",
            "        if (x == 3) {\n",
            "\n",
            "            try {\n",
            "                stmt\n",
            "                        .executeUpdate(\"UPDATE employee SET emp_name = 'madavan' where emp_id='20'; \");\n",
            "            } catch(Exception e){System.out.println(\"Exception \"+e);e.printStackTrace();} \n",
            "        }\n",
            "\n",
            "\n",
            "        query = \"SELECT * FROM employee \";\n",
            "        try {\n",
            "            rs = stmt.executeQuery(query);\n",
            "            // Check to see if any rows were read\n",
            "            more = rs.next();\n",
            "            if (!more) {\n",
            "\n",
            "                System.out.println(\"No rows found.\");\n",
            "                return;\n",
            "            }\n",
            "\n",
            "            // Loop through the rows retrieved from the query\n",
            "            while (more) {\n",
            "\n",
            "                v1 = \"ID: \" + rs.getInt(\"emp_id\");\n",
            "                v2 = \"Name: \" + rs.getString(\"emp_name\");\n",
            "\n",
            "                System.out.println(v1);\n",
            "                System.out.println(v2);\n",
            "                System.out.println(\"\");\n",
            "\n",
            "                results.addElement(v1 + \"\\n\" + v2 + \"\\n\");\n",
            "\n",
            "                more = rs.next();\n",
            "            }\n",
            "            rs.close();\n",
            "\n",
            "        } catch (SQLException e) {\n",
            "            System.out.println(\"\" + results.size() + \"results where found.\");\n",
            "        } \n",
            "        finally{stmt.close();}\n",
            "    }\n",
            "\n",
            "    public static void main(String[] args) throws SQLException {\n",
            "        String str = \"y\";\n",
            "        do {\n",
            "            DataBase s = new DataBase();\n",
            "            s.LoadDriver();\n",
            "            DataBase.connect();\n",
            "        Scanner sc = new Scanner(System.in);\n",
            "        System.out.println(\"DO u Want to PROCEED TO QUERY : \");\n",
            "        str = sc.next();\n",
            "        } while (str !=\"n\");\n",
            "    }\n",
            "\n",
            "}\n",
            "</code></pre>\n",
            "\n",
            "Score : 6\n",
            "Tags : java, jdbc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "AUeO_M6cBrTj",
        "outputId": "ef6eed3f-9ec2-4e0c-f2fa-a676a0318e0a"
      },
      "source": [
        "tags.head(10)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>Tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>80</td>\n",
              "      <td>flex</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>80</td>\n",
              "      <td>actionscript-3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>80</td>\n",
              "      <td>air</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>90</td>\n",
              "      <td>svn</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>90</td>\n",
              "      <td>tortoisesvn</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>90</td>\n",
              "      <td>branch</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>90</td>\n",
              "      <td>branching-and-merging</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>120</td>\n",
              "      <td>sql</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>120</td>\n",
              "      <td>asp.net</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>120</td>\n",
              "      <td>sitemap</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Id                    Tag\n",
              "0   80                   flex\n",
              "1   80         actionscript-3\n",
              "2   80                    air\n",
              "3   90                    svn\n",
              "4   90            tortoisesvn\n",
              "5   90                 branch\n",
              "6   90  branching-and-merging\n",
              "7  120                    sql\n",
              "8  120                asp.net\n",
              "9  120                sitemap"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcsznQ7bCY6b"
      },
      "source": [
        "tags = tags.groupby('Id').agg(lambda x: x.tolist())"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ni-R7DWgJv33"
      },
      "source": [
        "tags['Tag'] = tags['Tag'].str.join(\" \")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "pK6xxpkfRDJs",
        "outputId": "57b288ba-c123-47aa-c559-39c0f4806dff"
      },
      "source": [
        "tags.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tag</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Id</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>flex actionscript-3 air</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90</th>\n",
              "      <td>svn tortoisesvn branch branching-and-merging</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120</th>\n",
              "      <td>sql asp.net sitemap</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>180</th>\n",
              "      <td>algorithm language-agnostic colors color-space</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>260</th>\n",
              "      <td>c# .net scripting compiler-construction</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Tag\n",
              "Id                                                 \n",
              "80                          flex actionscript-3 air\n",
              "90     svn tortoisesvn branch branching-and-merging\n",
              "120                             sql asp.net sitemap\n",
              "180  algorithm language-agnostic colors color-space\n",
              "260         c# .net scripting compiler-construction"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "uwJEC_NNRV1B",
        "outputId": "6dadf785-0c3c-4caa-fec2-4cef68e8428d"
      },
      "source": [
        "questions.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>OwnerUserId</th>\n",
              "      <th>CreationDate</th>\n",
              "      <th>ClosedDate</th>\n",
              "      <th>Score</th>\n",
              "      <th>Title</th>\n",
              "      <th>Body</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>80</td>\n",
              "      <td>26.0</td>\n",
              "      <td>2008-08-01T13:57:07Z</td>\n",
              "      <td>NaN</td>\n",
              "      <td>26</td>\n",
              "      <td>SQLStatement.execute() - multiple queries in o...</td>\n",
              "      <td>&lt;p&gt;I've written a database generation script i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>90</td>\n",
              "      <td>58.0</td>\n",
              "      <td>2008-08-01T14:41:24Z</td>\n",
              "      <td>2012-12-26T03:45:49Z</td>\n",
              "      <td>144</td>\n",
              "      <td>Good branching and merging tutorials for Torto...</td>\n",
              "      <td>&lt;p&gt;Are there any really good tutorials explain...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>120</td>\n",
              "      <td>83.0</td>\n",
              "      <td>2008-08-01T15:50:08Z</td>\n",
              "      <td>NaN</td>\n",
              "      <td>21</td>\n",
              "      <td>ASP.NET Site Maps</td>\n",
              "      <td>&lt;p&gt;Has anyone got experience creating &lt;strong&gt;...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>180</td>\n",
              "      <td>2089740.0</td>\n",
              "      <td>2008-08-01T18:42:19Z</td>\n",
              "      <td>NaN</td>\n",
              "      <td>53</td>\n",
              "      <td>Function for creating color wheels</td>\n",
              "      <td>&lt;p&gt;This is something I've pseudo-solved many t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>260</td>\n",
              "      <td>91.0</td>\n",
              "      <td>2008-08-01T23:22:08Z</td>\n",
              "      <td>NaN</td>\n",
              "      <td>49</td>\n",
              "      <td>Adding scripting functionality to .NET applica...</td>\n",
              "      <td>&lt;p&gt;I have a little game written in C#. It uses...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Id  ...                                               Body\n",
              "0   80  ...  <p>I've written a database generation script i...\n",
              "1   90  ...  <p>Are there any really good tutorials explain...\n",
              "2  120  ...  <p>Has anyone got experience creating <strong>...\n",
              "3  180  ...  <p>This is something I've pseudo-solved many t...\n",
              "4  260  ...  <p>I have a little game written in C#. It uses...\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IaSbR5FXRG0x"
      },
      "source": [
        "df = pd.merge(questions, tags, on='Id')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "id": "34SiJswMRJKm",
        "outputId": "130da587-f65f-4fc3-8d93-c2a5a7938b3b"
      },
      "source": [
        "df.head(10)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>OwnerUserId</th>\n",
              "      <th>CreationDate</th>\n",
              "      <th>ClosedDate</th>\n",
              "      <th>Score</th>\n",
              "      <th>Title</th>\n",
              "      <th>Body</th>\n",
              "      <th>Tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>80</td>\n",
              "      <td>26.0</td>\n",
              "      <td>2008-08-01T13:57:07Z</td>\n",
              "      <td>NaN</td>\n",
              "      <td>26</td>\n",
              "      <td>SQLStatement.execute() - multiple queries in o...</td>\n",
              "      <td>&lt;p&gt;I've written a database generation script i...</td>\n",
              "      <td>flex actionscript-3 air</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>90</td>\n",
              "      <td>58.0</td>\n",
              "      <td>2008-08-01T14:41:24Z</td>\n",
              "      <td>2012-12-26T03:45:49Z</td>\n",
              "      <td>144</td>\n",
              "      <td>Good branching and merging tutorials for Torto...</td>\n",
              "      <td>&lt;p&gt;Are there any really good tutorials explain...</td>\n",
              "      <td>svn tortoisesvn branch branching-and-merging</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>120</td>\n",
              "      <td>83.0</td>\n",
              "      <td>2008-08-01T15:50:08Z</td>\n",
              "      <td>NaN</td>\n",
              "      <td>21</td>\n",
              "      <td>ASP.NET Site Maps</td>\n",
              "      <td>&lt;p&gt;Has anyone got experience creating &lt;strong&gt;...</td>\n",
              "      <td>sql asp.net sitemap</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>180</td>\n",
              "      <td>2089740.0</td>\n",
              "      <td>2008-08-01T18:42:19Z</td>\n",
              "      <td>NaN</td>\n",
              "      <td>53</td>\n",
              "      <td>Function for creating color wheels</td>\n",
              "      <td>&lt;p&gt;This is something I've pseudo-solved many t...</td>\n",
              "      <td>algorithm language-agnostic colors color-space</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>260</td>\n",
              "      <td>91.0</td>\n",
              "      <td>2008-08-01T23:22:08Z</td>\n",
              "      <td>NaN</td>\n",
              "      <td>49</td>\n",
              "      <td>Adding scripting functionality to .NET applica...</td>\n",
              "      <td>&lt;p&gt;I have a little game written in C#. It uses...</td>\n",
              "      <td>c# .net scripting compiler-construction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>330</td>\n",
              "      <td>63.0</td>\n",
              "      <td>2008-08-02T02:51:36Z</td>\n",
              "      <td>NaN</td>\n",
              "      <td>29</td>\n",
              "      <td>Should I use nested classes in this case?</td>\n",
              "      <td>&lt;p&gt;I am working on a collection of classes use...</td>\n",
              "      <td>c++ oop class nested-class</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>470</td>\n",
              "      <td>71.0</td>\n",
              "      <td>2008-08-02T15:11:47Z</td>\n",
              "      <td>2016-03-26T05:23:29Z</td>\n",
              "      <td>13</td>\n",
              "      <td>Homegrown consumption of web services</td>\n",
              "      <td>&lt;p&gt;I've been writing a few web services for a ...</td>\n",
              "      <td>.net web-services</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>580</td>\n",
              "      <td>91.0</td>\n",
              "      <td>2008-08-02T23:30:59Z</td>\n",
              "      <td>NaN</td>\n",
              "      <td>21</td>\n",
              "      <td>Deploying SQL Server Databases from Test to Live</td>\n",
              "      <td>&lt;p&gt;I wonder how you guys manage deployment of ...</td>\n",
              "      <td>sql-server sql-server-2005 deployment release-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>650</td>\n",
              "      <td>143.0</td>\n",
              "      <td>2008-08-03T11:12:52Z</td>\n",
              "      <td>NaN</td>\n",
              "      <td>79</td>\n",
              "      <td>Automatically update version number</td>\n",
              "      <td>&lt;p&gt;I would like the version property of my app...</td>\n",
              "      <td>c# visual-studio versioning</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>810</td>\n",
              "      <td>233.0</td>\n",
              "      <td>2008-08-03T20:35:01Z</td>\n",
              "      <td>NaN</td>\n",
              "      <td>9</td>\n",
              "      <td>Visual Studio Setup Project - Per User Registr...</td>\n",
              "      <td>&lt;p&gt;I'm trying to maintain a Setup Project in &lt;...</td>\n",
              "      <td>windows visual-studio registry installation</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Id  ...                                                Tag\n",
              "0   80  ...                            flex actionscript-3 air\n",
              "1   90  ...       svn tortoisesvn branch branching-and-merging\n",
              "2  120  ...                                sql asp.net sitemap\n",
              "3  180  ...     algorithm language-agnostic colors color-space\n",
              "4  260  ...            c# .net scripting compiler-construction\n",
              "5  330  ...                         c++ oop class nested-class\n",
              "6  470  ...                                  .net web-services\n",
              "7  580  ...  sql-server sql-server-2005 deployment release-...\n",
              "8  650  ...                        c# visual-studio versioning\n",
              "9  810  ...        windows visual-studio registry installation\n",
              "\n",
              "[10 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uToJHDP4TIoP",
        "outputId": "e881cfd5-267c-4cda-d8ae-258cbc69f0dd"
      },
      "source": [
        "def removeHtmlTags(data):\n",
        "    cleaner = re.compile('<.*?>')\n",
        "    clean_text = re.sub(cleaner,\" \",str(data))\n",
        "    return clean_text\n",
        "sent = \"<html><p>hello world</p></html>\"\n",
        "rece = removeHtmlTags(sent)\n",
        "print(rece)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  hello world  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSBJVjWSVD0A"
      },
      "source": [
        "preprossed_data = []\n",
        "questions_proccesed = 0\n",
        "data_size = 50000"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0sIuVLqCVG07",
        "outputId": "1d170a35-9917-4843-8a92-da084606b65b"
      },
      "source": [
        "start = datetime.now()\n",
        "questions_with_code=0\n",
        "for i in range(min(data_size, len(questions))):\n",
        "    is_code = 0\n",
        "\n",
        "    title = df['Title'][i]\n",
        "    question = df['Body'][i]\n",
        "    tag = df['Tag'][i]\n",
        "    \n",
        "    if '<code>' in question:\n",
        "        questions_with_code+=1\n",
        "        is_code = 1\n",
        "    x = len(question)+len(title)\n",
        "\n",
        "    code = str(re.findall(r'<code>(.*?)</code>', question, flags=re.DOTALL))\n",
        "\n",
        "    question=re.sub('<code>(.*?)</code>', '', question, flags=re.MULTILINE|re.DOTALL)\n",
        "    question=removeHtmlTags(question.encode('utf-8'))\n",
        "\n",
        "    title=title.encode('utf-8')\n",
        "\n",
        "    question=str(title)+\" \"+str(question)\n",
        "    question=re.sub(r'[^A-Za-z]+',' ',question)\n",
        "    words=word_tokenize(str(question.lower()))\n",
        "    question=' '.join(str(stemmer.stem(j)) for j in words if j not in stop_words and (len(j)!=1 or j=='c'))\n",
        "    tup = (question,code,is_code,tag)\n",
        "    preprossed_data.append(tup)\n",
        "    questions_proccesed += 1\n",
        "    if (questions_proccesed%10000==0):\n",
        "        print(\"number of questions completed=\",questions_proccesed)\n",
        "    \n",
        "print(\"Time take to execute this cell \", datetime.now()-start)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of questions completed= 10000\n",
            "number of questions completed= 20000\n",
            "number of questions completed= 30000\n",
            "number of questions completed= 40000\n",
            "number of questions completed= 50000\n",
            "Time take to execute this cell  0:01:08.841446\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzBjHtzVV2Aq",
        "outputId": "c8d90a2f-b8e0-40e1-9c99-5bbd330d6072"
      },
      "source": [
        "preprossed_data[25]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('travers collect classic asp want abl thing thing nend classic asp net',\n",
              " '[]',\n",
              " 0,\n",
              " 'asp-classic vbscript')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOXkOLmoWsi0"
      },
      "source": [
        "data = pd.DataFrame(preprossed_data[:data_size], columns =['question', 'code', 'is_code','tag'])\n",
        "data.drop(['is_code','code'], axis =1, inplace=True)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "aIDQ4dInXCTv",
        "outputId": "5bac3296-1b84-4eeb-bbe9-a5cadd0b2a91"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>sqlstatement execut multipl queri one statemen...</td>\n",
              "      <td>flex actionscript-3 air</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>good branch merg tutori tortoisesvn realli goo...</td>\n",
              "      <td>svn tortoisesvn branch branching-and-merging</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>asp net site map anyon got experi creat sql ba...</td>\n",
              "      <td>sql asp.net sitemap</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>function creat color wheel someth pseudo solv ...</td>\n",
              "      <td>algorithm language-agnostic colors color-space</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ad script function net applic littl game writt...</td>\n",
              "      <td>c# .net scripting compiler-construction</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            question                                             tag\n",
              "0  sqlstatement execut multipl queri one statemen...                         flex actionscript-3 air\n",
              "1  good branch merg tutori tortoisesvn realli goo...    svn tortoisesvn branch branching-and-merging\n",
              "2  asp net site map anyon got experi creat sql ba...                             sql asp.net sitemap\n",
              "3  function creat color wheel someth pseudo solv ...  algorithm language-agnostic colors color-space\n",
              "4  ad script function net applic littl game writt...         c# .net scripting compiler-construction"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrlcNflzXEZ_"
      },
      "source": [
        "vectorizer = CountVectorizer(tokenizer = lambda x: x.split(), binary='true')\n",
        "Y = vectorizer.fit_transform(data['tag'].values.astype('U'))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xy4w7TPJFc2L",
        "outputId": "27d383d2-2f36-493d-d23e-f66c0d11e896"
      },
      "source": [
        "print(vectorizer.get_feature_names()[75:85])"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['abl deal', 'abl debug', 'abl decid', 'abl declar', 'abl defin', 'abl delet', 'abl deploy', 'abl detect', 'abl determin', 'abl develop']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgQUZuSsX1LK"
      },
      "source": [
        "def tags_to_choose(n):\n",
        "    t = Y.sum(axis=0).tolist()[0]\n",
        "    sorted_tags_i = sorted(range(len(t)), key=lambda i: t[i], reverse=True)\n",
        "    multilabel_yn=Y[:,sorted_tags_i[:n]]\n",
        "    return multilabel_yn\n",
        "\n",
        "def questions_covered_fn(n):\n",
        "    multilabel_yn = tags_to_choose(n)\n",
        "    x= multilabel_yn.sum(axis=1)\n",
        "    return (np.count_nonzero(x==0))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCctKkvFaSR-"
      },
      "source": [
        "questions_covered = []\n",
        "total_tags=Y.shape[1]\n",
        "total_qs=data.shape[0]\n",
        "for i in range(50, total_tags, 50):\n",
        "    questions_covered.append(np.round(((total_qs-questions_covered_fn(i))/total_qs)*100,3))"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEily0hXjeEl",
        "outputId": "45390395-c310-41a8-926b-78bb1d5cfd0d"
      },
      "source": [
        "questions_covered[:10]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[78.016, 85.97, 88.86, 91.136, 92.446, 93.572, 94.404, 95.196, 95.676, 96.074]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "Y6bniMwiafAM",
        "outputId": "31aa9e40-54a3-483c-c31b-6656f4118619"
      },
      "source": [
        "xlabel = [i for i in range(50,total_tags,50)]\n",
        "plt.plot(xlabel,questions_covered)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fba22b78c50>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rr_geRcEatey",
        "outputId": "f21bcb4b-be1c-4149-c386-2890598b36f2"
      },
      "source": [
        "multilabel_yx = tags_to_choose(200)\n",
        "print(\"number of questions that are not covered :\", questions_covered_fn(200),\"out of \", total_qs)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of questions that are not covered : 4432 out of  50000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-MH-IQVa1VB",
        "outputId": "9eff3eaa-93fa-4ff5-e71d-e66d0080e81a"
      },
      "source": [
        "print(\"Number of tags in sample :\", Y.shape[1])\n",
        "print(\"number of tags taken :\", multilabel_yx.shape[1],\"(\",(multilabel_yx.shape[1]/Y.shape[1])*100,\"%)\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of tags in sample : 9776\n",
            "number of tags taken : 200 ( 2.0458265139116203 %)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bB34bhWha4c7"
      },
      "source": [
        "total_size=data.shape[0]\n",
        "train_size=int(0.80*total_size)\n",
        "\n",
        "x_train=data.head(train_size)\n",
        "x_test=data.tail(total_size - train_size)\n",
        "\n",
        "y_train = multilabel_yx[0:train_size,:]\n",
        "y_test = multilabel_yx[train_size:total_size,:]"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itPHhYtKbC2S",
        "outputId": "01b53034-7222-4f35-c4d7-783dd253666d"
      },
      "source": [
        "print(\"Number of data points in train data :\", y_train.shape)\n",
        "print(\"Number of data points in test data :\", y_test.shape)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of data points in train data : (40000, 200)\n",
            "Number of data points in test data : (10000, 200)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJgTlBpLbLBc",
        "outputId": "faef6967-60ea-4989-9401-632840c77ebf"
      },
      "source": [
        "start = datetime.now()\n",
        "vectorizer = TfidfVectorizer(min_df=0.00009, max_features=200000, smooth_idf=True, norm=\"l2\", \\\n",
        "                             tokenizer = lambda x: x.split(), sublinear_tf=False, ngram_range=(1,3))\n",
        "x_train_multilabel = vectorizer.fit_transform(x_train['question'])\n",
        "x_test_multilabel = vectorizer.transform(x_test['question'])\n",
        "print(\"Time taken to run this cell :\", datetime.now() - start)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time taken to run this cell : 0:00:32.043933\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBraVx-wbPDB",
        "outputId": "9e7c2eab-068a-4fdb-de55-507eef778c58"
      },
      "source": [
        "classifier = OneVsRestClassifier(SGDClassifier(loss='log', alpha=0.00001, penalty='l1'), n_jobs=-1)\n",
        "classifier.fit(x_train_multilabel, y_train)\n",
        "predictions = classifier.predict(x_test_multilabel)\n",
        "\n",
        "print(\"accuracy :\",metrics.accuracy_score(y_test,predictions))\n",
        "print(\"macro f1 score :\",metrics.f1_score(y_test, predictions, average = 'macro'))\n",
        "print(\"micro f1 scoore :\",metrics.f1_score(y_test, predictions, average = 'micro'))\n",
        "print(\"hamming loss :\",metrics.hamming_loss(y_test,predictions))\n",
        "print(\"Precision recall report :\\n\",metrics.classification_report(y_test, predictions))\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy : 0.2834\n",
            "macro f1 score : 0.41525835519822735\n",
            "micro f1 scoore : 0.5395811956067343\n",
            "hamming loss : 0.0060575\n",
            "Precision recall report :\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.31      0.41      1116\n",
            "           1       0.80      0.58      0.67       802\n",
            "           2       0.54      0.17      0.26       540\n",
            "           3       0.84      0.63      0.72       743\n",
            "           4       0.80      0.49      0.61       484\n",
            "           5       0.73      0.51      0.60       556\n",
            "           6       0.69      0.34      0.46       449\n",
            "           7       0.89      0.74      0.81       520\n",
            "           8       0.87      0.60      0.71       406\n",
            "           9       0.91      0.66      0.77       483\n",
            "          10       0.57      0.39      0.46       287\n",
            "          11       0.56      0.28      0.38       318\n",
            "          12       0.58      0.42      0.49       186\n",
            "          13       0.85      0.65      0.74       288\n",
            "          14       0.83      0.68      0.75       200\n",
            "          15       0.61      0.25      0.35       195\n",
            "          16       0.95      0.71      0.82       214\n",
            "          17       0.78      0.55      0.65       221\n",
            "          18       0.86      0.73      0.79       163\n",
            "          19       0.59      0.27      0.37       218\n",
            "          20       0.26      0.08      0.12       131\n",
            "          21       0.72      0.49      0.58       145\n",
            "          22       0.66      0.63      0.65       145\n",
            "          23       0.45      0.25      0.32       102\n",
            "          24       0.17      0.05      0.08       115\n",
            "          25       0.84      0.40      0.55       126\n",
            "          26       0.63      0.39      0.48        95\n",
            "          27       0.92      0.72      0.81       124\n",
            "          28       0.71      0.45      0.55       127\n",
            "          29       0.98      0.78      0.87       162\n",
            "          30       0.53      0.32      0.40        79\n",
            "          31       0.49      0.19      0.28       103\n",
            "          32       0.38      0.11      0.17        81\n",
            "          33       0.61      0.49      0.55        93\n",
            "          34       0.89      0.47      0.61        90\n",
            "          35       0.97      0.69      0.81       239\n",
            "          36       0.70      0.42      0.53        66\n",
            "          37       0.35      0.10      0.16        68\n",
            "          38       0.75      0.41      0.53       100\n",
            "          39       0.42      0.25      0.32        83\n",
            "          40       0.83      0.46      0.59        83\n",
            "          41       0.70      0.24      0.35        97\n",
            "          42       0.92      0.74      0.82        73\n",
            "          43       0.50      0.18      0.26       102\n",
            "          44       0.50      0.27      0.35        52\n",
            "          45       0.15      0.09      0.11        57\n",
            "          46       0.57      0.35      0.43        60\n",
            "          47       0.90      0.64      0.75        73\n",
            "          48       0.63      0.44      0.52        87\n",
            "          49       0.93      0.52      0.67        96\n",
            "          50       0.92      0.75      0.83        64\n",
            "          51       0.59      0.29      0.39        75\n",
            "          52       0.74      0.69      0.72        84\n",
            "          53       0.93      0.60      0.73        70\n",
            "          54       0.53      0.15      0.23        62\n",
            "          55       0.70      0.54      0.61        59\n",
            "          56       0.74      0.69      0.72        62\n",
            "          57       0.92      0.78      0.84        69\n",
            "          58       0.77      0.56      0.65        54\n",
            "          59       0.60      0.06      0.10        54\n",
            "          60       0.55      0.17      0.26        66\n",
            "          61       0.74      0.52      0.61        48\n",
            "          62       0.39      0.26      0.32        57\n",
            "          63       0.71      0.19      0.30        52\n",
            "          64       0.52      0.31      0.39        54\n",
            "          65       0.33      0.09      0.14        57\n",
            "          66       0.66      0.62      0.64        34\n",
            "          67       0.43      0.09      0.15        67\n",
            "          68       0.17      0.03      0.05        36\n",
            "          69       0.56      0.41      0.47        37\n",
            "          70       0.00      0.00      0.00        49\n",
            "          71       0.00      0.00      0.00        35\n",
            "          72       0.56      0.34      0.42        53\n",
            "          73       0.22      0.12      0.16        33\n",
            "          74       0.55      0.12      0.20        49\n",
            "          75       0.53      0.28      0.36        36\n",
            "          76       0.75      0.51      0.61        35\n",
            "          77       0.50      0.14      0.22        42\n",
            "          78       0.83      0.64      0.72        53\n",
            "          79       0.53      0.19      0.28        42\n",
            "          80       0.94      0.80      0.86        64\n",
            "          81       0.38      0.17      0.23        36\n",
            "          82       0.86      0.31      0.46        58\n",
            "          83       0.55      0.34      0.42        35\n",
            "          84       0.54      0.49      0.51        41\n",
            "          85       0.93      0.69      0.79        58\n",
            "          86       0.61      0.35      0.44        40\n",
            "          87       0.36      0.21      0.27        47\n",
            "          88       0.45      0.12      0.20        40\n",
            "          89       0.43      0.21      0.28        43\n",
            "          90       0.81      0.63      0.71        41\n",
            "          91       0.68      0.44      0.54        34\n",
            "          92       0.83      0.35      0.49        54\n",
            "          93       0.68      0.59      0.63        39\n",
            "          94       0.46      0.20      0.28        30\n",
            "          95       0.25      0.13      0.17        23\n",
            "          96       0.37      0.28      0.32        36\n",
            "          97       0.84      0.66      0.74        47\n",
            "          98       0.78      0.45      0.57        40\n",
            "          99       0.89      0.54      0.68        46\n",
            "         100       0.52      0.38      0.44        29\n",
            "         101       0.83      0.43      0.57        35\n",
            "         102       0.97      0.55      0.70        51\n",
            "         103       0.53      0.26      0.35        35\n",
            "         104       0.42      0.16      0.23        31\n",
            "         105       0.67      0.50      0.57        28\n",
            "         106       0.94      0.64      0.76        45\n",
            "         107       0.00      0.00      0.00        42\n",
            "         108       0.20      0.04      0.06        26\n",
            "         109       0.25      0.07      0.11        28\n",
            "         110       0.62      0.14      0.23        36\n",
            "         111       0.86      0.62      0.72        40\n",
            "         112       0.50      0.13      0.21        30\n",
            "         113       0.33      0.12      0.18        16\n",
            "         114       0.67      0.09      0.15        23\n",
            "         115       0.23      0.10      0.14        29\n",
            "         116       0.62      0.30      0.40        27\n",
            "         117       0.90      0.70      0.79        37\n",
            "         118       0.33      0.06      0.11        31\n",
            "         119       0.43      0.35      0.38        26\n",
            "         120       0.55      0.17      0.26        36\n",
            "         121       0.48      0.32      0.38        31\n",
            "         122       0.27      0.07      0.12        40\n",
            "         123       0.14      0.02      0.04        43\n",
            "         124       0.25      0.10      0.15        29\n",
            "         125       0.22      0.06      0.09        36\n",
            "         126       0.60      0.43      0.50        28\n",
            "         127       0.60      0.17      0.26        18\n",
            "         128       0.28      0.16      0.20        31\n",
            "         129       0.00      0.00      0.00        18\n",
            "         130       0.00      0.00      0.00        35\n",
            "         131       0.58      0.28      0.38        25\n",
            "         132       0.20      0.07      0.10        30\n",
            "         133       0.70      0.76      0.73        21\n",
            "         134       0.18      0.05      0.08        41\n",
            "         135       0.55      0.19      0.28        32\n",
            "         136       0.46      0.21      0.29        29\n",
            "         137       0.50      0.43      0.47        23\n",
            "         138       0.82      0.48      0.61        29\n",
            "         139       0.92      0.50      0.65        24\n",
            "         140       0.96      0.67      0.79        33\n",
            "         141       0.80      0.73      0.76        33\n",
            "         142       0.33      0.11      0.17        18\n",
            "         143       0.71      0.53      0.61        19\n",
            "         144       1.00      0.12      0.22        49\n",
            "         145       0.43      0.18      0.26        33\n",
            "         146       0.29      0.06      0.10        34\n",
            "         147       0.86      0.43      0.57        14\n",
            "         148       0.44      0.29      0.35        14\n",
            "         149       0.87      0.59      0.70        22\n",
            "         150       0.44      0.14      0.22        28\n",
            "         151       0.33      0.29      0.31        24\n",
            "         152       0.25      0.04      0.06        27\n",
            "         153       0.73      0.42      0.54        26\n",
            "         154       0.00      0.00      0.00        20\n",
            "         155       0.88      0.56      0.68        25\n",
            "         156       0.92      0.60      0.73        20\n",
            "         157       0.60      0.41      0.49        22\n",
            "         158       0.71      0.38      0.50        26\n",
            "         159       0.40      0.14      0.21        28\n",
            "         160       0.44      0.32      0.37        22\n",
            "         161       0.33      0.07      0.12        28\n",
            "         162       0.67      0.48      0.56        25\n",
            "         163       0.50      0.11      0.18        18\n",
            "         164       0.74      0.68      0.71        25\n",
            "         165       0.29      0.10      0.15        20\n",
            "         166       0.88      0.70      0.78        53\n",
            "         167       0.71      0.29      0.41        35\n",
            "         168       0.77      0.50      0.61        20\n",
            "         169       0.50      0.41      0.45        17\n",
            "         170       0.75      0.13      0.22        23\n",
            "         171       0.67      0.25      0.36        24\n",
            "         172       0.00      0.00      0.00        21\n",
            "         173       0.67      0.53      0.59        15\n",
            "         174       1.00      0.16      0.27        19\n",
            "         175       0.50      0.50      0.50        12\n",
            "         176       0.70      0.32      0.44        44\n",
            "         177       0.33      0.14      0.19        22\n",
            "         178       0.79      0.69      0.73        32\n",
            "         179       0.00      0.00      0.00        12\n",
            "         180       0.00      0.00      0.00        12\n",
            "         181       0.33      0.04      0.07        26\n",
            "         182       0.43      0.43      0.43        14\n",
            "         183       0.73      0.48      0.58        23\n",
            "         184       0.59      0.71      0.65        14\n",
            "         185       1.00      0.06      0.11        17\n",
            "         186       0.75      0.60      0.67        30\n",
            "         187       0.67      0.32      0.43        25\n",
            "         188       0.22      0.18      0.20        11\n",
            "         189       0.00      0.00      0.00         8\n",
            "         190       0.00      0.00      0.00        23\n",
            "         191       1.00      0.86      0.92        28\n",
            "         192       0.94      0.54      0.68        28\n",
            "         193       0.18      0.09      0.12        22\n",
            "         194       0.70      0.30      0.42        23\n",
            "         195       0.83      0.50      0.62        30\n",
            "         196       0.94      0.68      0.79        22\n",
            "         197       0.00      0.00      0.00        15\n",
            "         198       0.25      0.06      0.09        18\n",
            "         199       0.00      0.00      0.00        12\n",
            "\n",
            "   micro avg       0.73      0.43      0.54     16530\n",
            "   macro avg       0.58      0.34      0.42     16530\n",
            "weighted avg       0.68      0.43      0.52     16530\n",
            " samples avg       0.51      0.43      0.44     16530\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lPNbcEQC_sP",
        "outputId": "6baab9a9-852c-4214-e017-77f0cccfb314"
      },
      "source": [
        "classifier = OneVsRestClassifier(LogisticRegression(penalty='l2'), n_jobs=-1)\n",
        "classifier.fit(x_train_multilabel, y_train)\n",
        "predictions = classifier.predict(x_test_multilabel)\n",
        "\n",
        "print(\"accuracy :\",metrics.accuracy_score(y_test,predictions))\n",
        "print(\"macro f1 score :\",metrics.f1_score(y_test, predictions, average = 'macro'))\n",
        "print(\"micro f1 scoore :\",metrics.f1_score(y_test, predictions, average = 'micro'))\n",
        "print(\"hamming loss :\",metrics.hamming_loss(y_test,predictions))\n",
        "print(\"Precision recall report :\\n\",metrics.classification_report(y_test, predictions))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy : 0.2092\n",
            "macro f1 score : 0.18908139175141372\n",
            "micro f1 scoore : 0.36396852164137156\n",
            "hamming loss : 0.006789\n",
            "Precision recall report :\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.19      0.30      1116\n",
            "           1       0.83      0.41      0.55       802\n",
            "           2       0.66      0.09      0.16       540\n",
            "           3       0.90      0.47      0.62       743\n",
            "           4       0.82      0.34      0.48       484\n",
            "           5       0.77      0.37      0.50       556\n",
            "           6       0.74      0.19      0.30       449\n",
            "           7       0.91      0.57      0.70       520\n",
            "           8       0.89      0.36      0.51       406\n",
            "           9       0.94      0.39      0.55       483\n",
            "          10       0.65      0.28      0.39       287\n",
            "          11       0.64      0.16      0.26       318\n",
            "          12       0.56      0.28      0.38       186\n",
            "          13       0.91      0.47      0.62       288\n",
            "          14       0.89      0.53      0.66       200\n",
            "          15       0.65      0.07      0.12       195\n",
            "          16       0.95      0.37      0.53       214\n",
            "          17       0.86      0.38      0.52       221\n",
            "          18       0.93      0.50      0.65       163\n",
            "          19       0.82      0.08      0.15       218\n",
            "          20       0.33      0.03      0.06       131\n",
            "          21       0.80      0.24      0.37       145\n",
            "          22       0.71      0.45      0.55       145\n",
            "          23       0.49      0.17      0.25       102\n",
            "          24       0.20      0.02      0.03       115\n",
            "          25       0.97      0.22      0.36       126\n",
            "          26       0.60      0.13      0.21        95\n",
            "          27       0.97      0.52      0.67       124\n",
            "          28       0.71      0.25      0.37       127\n",
            "          29       0.99      0.43      0.60       162\n",
            "          30       0.62      0.10      0.17        79\n",
            "          31       0.67      0.02      0.04       103\n",
            "          32       0.50      0.09      0.15        81\n",
            "          33       0.69      0.40      0.50        93\n",
            "          34       0.89      0.18      0.30        90\n",
            "          35       0.98      0.18      0.31       239\n",
            "          36       0.71      0.23      0.34        66\n",
            "          37       0.40      0.03      0.05        68\n",
            "          38       0.92      0.24      0.38       100\n",
            "          39       0.48      0.12      0.19        83\n",
            "          40       0.88      0.25      0.39        83\n",
            "          41       1.00      0.04      0.08        97\n",
            "          42       0.94      0.42      0.58        73\n",
            "          43       0.60      0.03      0.06       102\n",
            "          44       0.60      0.12      0.19        52\n",
            "          45       0.00      0.00      0.00        57\n",
            "          46       0.90      0.15      0.26        60\n",
            "          47       1.00      0.29      0.45        73\n",
            "          48       0.66      0.24      0.35        87\n",
            "          49       0.93      0.27      0.42        96\n",
            "          50       0.93      0.41      0.57        64\n",
            "          51       0.60      0.04      0.07        75\n",
            "          52       0.74      0.38      0.50        84\n",
            "          53       0.96      0.31      0.47        70\n",
            "          54       0.00      0.00      0.00        62\n",
            "          55       0.69      0.41      0.51        59\n",
            "          56       0.65      0.27      0.39        62\n",
            "          57       0.88      0.41      0.55        69\n",
            "          58       0.84      0.30      0.44        54\n",
            "          59       0.00      0.00      0.00        54\n",
            "          60       0.00      0.00      0.00        66\n",
            "          61       0.88      0.29      0.44        48\n",
            "          62       0.25      0.05      0.09        57\n",
            "          63       0.00      0.00      0.00        52\n",
            "          64       0.67      0.07      0.13        54\n",
            "          65       0.25      0.02      0.03        57\n",
            "          66       0.80      0.35      0.49        34\n",
            "          67       1.00      0.03      0.06        67\n",
            "          68       0.00      0.00      0.00        36\n",
            "          69       0.60      0.08      0.14        37\n",
            "          70       0.00      0.00      0.00        49\n",
            "          71       0.00      0.00      0.00        35\n",
            "          72       0.60      0.17      0.26        53\n",
            "          73       0.67      0.06      0.11        33\n",
            "          74       0.00      0.00      0.00        49\n",
            "          75       1.00      0.06      0.11        36\n",
            "          76       0.80      0.11      0.20        35\n",
            "          77       0.00      0.00      0.00        42\n",
            "          78       1.00      0.38      0.55        53\n",
            "          79       0.60      0.07      0.13        42\n",
            "          80       0.96      0.42      0.59        64\n",
            "          81       0.00      0.00      0.00        36\n",
            "          82       0.75      0.05      0.10        58\n",
            "          83       1.00      0.06      0.11        35\n",
            "          84       0.44      0.17      0.25        41\n",
            "          85       1.00      0.22      0.37        58\n",
            "          86       0.50      0.03      0.05        40\n",
            "          87       0.40      0.04      0.08        47\n",
            "          88       0.00      0.00      0.00        40\n",
            "          89       0.50      0.02      0.04        43\n",
            "          90       0.88      0.17      0.29        41\n",
            "          91       0.83      0.15      0.25        34\n",
            "          92       1.00      0.02      0.04        54\n",
            "          93       0.77      0.26      0.38        39\n",
            "          94       0.67      0.13      0.22        30\n",
            "          95       0.25      0.04      0.07        23\n",
            "          96       0.60      0.08      0.15        36\n",
            "          97       0.88      0.15      0.25        47\n",
            "          98       0.83      0.12      0.22        40\n",
            "          99       0.89      0.17      0.29        46\n",
            "         100       0.60      0.21      0.31        29\n",
            "         101       1.00      0.11      0.21        35\n",
            "         102       1.00      0.20      0.33        51\n",
            "         103       0.50      0.03      0.05        35\n",
            "         104       1.00      0.03      0.06        31\n",
            "         105       1.00      0.11      0.19        28\n",
            "         106       1.00      0.29      0.45        45\n",
            "         107       0.00      0.00      0.00        42\n",
            "         108       0.00      0.00      0.00        26\n",
            "         109       0.00      0.00      0.00        28\n",
            "         110       0.00      0.00      0.00        36\n",
            "         111       0.80      0.10      0.18        40\n",
            "         112       0.00      0.00      0.00        30\n",
            "         113       0.00      0.00      0.00        16\n",
            "         114       0.00      0.00      0.00        23\n",
            "         115       0.00      0.00      0.00        29\n",
            "         116       1.00      0.07      0.14        27\n",
            "         117       0.90      0.24      0.38        37\n",
            "         118       0.00      0.00      0.00        31\n",
            "         119       0.50      0.08      0.13        26\n",
            "         120       0.67      0.06      0.10        36\n",
            "         121       0.67      0.13      0.22        31\n",
            "         122       0.00      0.00      0.00        40\n",
            "         123       0.00      0.00      0.00        43\n",
            "         124       0.50      0.03      0.06        29\n",
            "         125       0.00      0.00      0.00        36\n",
            "         126       0.00      0.00      0.00        28\n",
            "         127       0.00      0.00      0.00        18\n",
            "         128       0.00      0.00      0.00        31\n",
            "         129       0.00      0.00      0.00        18\n",
            "         130       0.00      0.00      0.00        35\n",
            "         131       0.00      0.00      0.00        25\n",
            "         132       0.00      0.00      0.00        30\n",
            "         133       0.80      0.19      0.31        21\n",
            "         134       0.00      0.00      0.00        41\n",
            "         135       0.50      0.03      0.06        32\n",
            "         136       1.00      0.10      0.19        29\n",
            "         137       0.60      0.26      0.36        23\n",
            "         138       1.00      0.07      0.13        29\n",
            "         139       1.00      0.04      0.08        24\n",
            "         140       1.00      0.09      0.17        33\n",
            "         141       0.86      0.18      0.30        33\n",
            "         142       0.00      0.00      0.00        18\n",
            "         143       0.67      0.11      0.18        19\n",
            "         144       0.00      0.00      0.00        49\n",
            "         145       0.67      0.06      0.11        33\n",
            "         146       0.00      0.00      0.00        34\n",
            "         147       1.00      0.14      0.25        14\n",
            "         148       0.50      0.07      0.12        14\n",
            "         149       1.00      0.27      0.43        22\n",
            "         150       0.00      0.00      0.00        28\n",
            "         151       0.29      0.08      0.13        24\n",
            "         152       0.00      0.00      0.00        27\n",
            "         153       0.00      0.00      0.00        26\n",
            "         154       0.00      0.00      0.00        20\n",
            "         155       0.00      0.00      0.00        25\n",
            "         156       1.00      0.25      0.40        20\n",
            "         157       0.00      0.00      0.00        22\n",
            "         158       0.50      0.04      0.07        26\n",
            "         159       0.00      0.00      0.00        28\n",
            "         160       0.00      0.00      0.00        22\n",
            "         161       0.00      0.00      0.00        28\n",
            "         162       0.83      0.20      0.32        25\n",
            "         163       0.00      0.00      0.00        18\n",
            "         164       1.00      0.20      0.33        25\n",
            "         165       0.00      0.00      0.00        20\n",
            "         166       1.00      0.15      0.26        53\n",
            "         167       0.00      0.00      0.00        35\n",
            "         168       0.80      0.20      0.32        20\n",
            "         169       0.50      0.06      0.11        17\n",
            "         170       0.00      0.00      0.00        23\n",
            "         171       1.00      0.12      0.22        24\n",
            "         172       0.00      0.00      0.00        21\n",
            "         173       1.00      0.07      0.12        15\n",
            "         174       0.00      0.00      0.00        19\n",
            "         175       1.00      0.17      0.29        12\n",
            "         176       0.00      0.00      0.00        44\n",
            "         177       0.50      0.05      0.08        22\n",
            "         178       1.00      0.09      0.17        32\n",
            "         179       0.00      0.00      0.00        12\n",
            "         180       0.00      0.00      0.00        12\n",
            "         181       0.00      0.00      0.00        26\n",
            "         182       0.67      0.14      0.24        14\n",
            "         183       0.50      0.04      0.08        23\n",
            "         184       0.50      0.07      0.12        14\n",
            "         185       0.00      0.00      0.00        17\n",
            "         186       0.70      0.23      0.35        30\n",
            "         187       1.00      0.04      0.08        25\n",
            "         188       0.50      0.09      0.15        11\n",
            "         189       0.00      0.00      0.00         8\n",
            "         190       0.00      0.00      0.00        23\n",
            "         191       1.00      0.07      0.13        28\n",
            "         192       1.00      0.29      0.44        28\n",
            "         193       0.00      0.00      0.00        22\n",
            "         194       1.00      0.04      0.08        23\n",
            "         195       1.00      0.03      0.06        30\n",
            "         196       0.00      0.00      0.00        22\n",
            "         197       0.00      0.00      0.00        15\n",
            "         198       1.00      0.06      0.11        18\n",
            "         199       0.00      0.00      0.00        12\n",
            "\n",
            "   micro avg       0.81      0.24      0.36     16530\n",
            "   macro avg       0.53      0.13      0.19     16530\n",
            "weighted avg       0.69      0.24      0.33     16530\n",
            " samples avg       0.32      0.24      0.26     16530\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5wvogbbbfQV"
      },
      "source": [
        "# classifier = OneVsRestClassifier(svm.SVC(kernel='linear'))\n",
        "# classifier.fit(x_train_multilabel, y_train)\n",
        "# predictions = classifier.predict(x_test_multilabel)\n",
        "\n",
        "# print(\"accuracy :\",metrics.accuracy_score(y_test,predictions))\n",
        "# print(\"macro f1 score :\",metrics.f1_score(y_test, predictions, average = 'macro'))\n",
        "# print(\"micro f1 scoore :\",metrics.f1_score(y_test, predictions, average = 'micro'))\n",
        "# print(\"hamming loss :\",metrics.hamming_loss(y_test,predictions))\n",
        "# print(\"Precision recall report :\\n\",metrics.classification_report(y_test, predictions))"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwK3YIHDDTUV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b3c5f38-e344-4596-fafd-8de5d7408156"
      },
      "source": [
        "classifier = OneVsRestClassifier(SGDClassifier(loss='hinge', alpha=0.00001, penalty='l1'), n_jobs=-1)\n",
        "classifier.fit(x_train_multilabel, y_train)\n",
        "predictions = classifier.predict(x_test_multilabel)\n",
        "\n",
        "print(\"accuracy :\",metrics.accuracy_score(y_test,predictions))\n",
        "print(\"macro f1 score :\",metrics.f1_score(y_test, predictions, average = 'macro'))\n",
        "print(\"micro f1 scoore :\",metrics.f1_score(y_test, predictions, average = 'micro'))\n",
        "print(\"hamming loss :\",metrics.hamming_loss(y_test,predictions))\n",
        "print(\"Precision recall report :\\n\",metrics.classification_report(y_test, predictions))\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy : 0.3139\n",
            "macro f1 score : 0.4029361817742817\n",
            "micro f1 scoore : 0.5695957487189219\n",
            "hamming loss : 0.0056695\n",
            "Precision recall report :\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.34      0.43      1116\n",
            "           1       0.79      0.61      0.69       802\n",
            "           2       0.53      0.16      0.25       540\n",
            "           3       0.84      0.67      0.74       743\n",
            "           4       0.80      0.52      0.63       484\n",
            "           5       0.76      0.55      0.64       556\n",
            "           6       0.70      0.36      0.48       449\n",
            "           7       0.88      0.82      0.85       520\n",
            "           8       0.90      0.68      0.78       406\n",
            "           9       0.91      0.75      0.82       483\n",
            "          10       0.63      0.36      0.46       287\n",
            "          11       0.59      0.28      0.38       318\n",
            "          12       0.59      0.39      0.47       186\n",
            "          13       0.86      0.69      0.77       288\n",
            "          14       0.86      0.73      0.79       200\n",
            "          15       0.61      0.18      0.28       195\n",
            "          16       0.95      0.77      0.85       214\n",
            "          17       0.76      0.58      0.66       221\n",
            "          18       0.87      0.80      0.83       163\n",
            "          19       0.65      0.30      0.41       218\n",
            "          20       0.33      0.01      0.01       131\n",
            "          21       0.77      0.59      0.66       145\n",
            "          22       0.67      0.66      0.66       145\n",
            "          23       0.53      0.24      0.33       102\n",
            "          24       0.25      0.01      0.02       115\n",
            "          25       0.89      0.50      0.64       126\n",
            "          26       0.67      0.45      0.54        95\n",
            "          27       0.90      0.76      0.82       124\n",
            "          28       0.73      0.54      0.62       127\n",
            "          29       0.95      0.90      0.92       162\n",
            "          30       0.53      0.32      0.40        79\n",
            "          31       0.78      0.07      0.12       103\n",
            "          32       0.25      0.01      0.02        81\n",
            "          33       0.58      0.56      0.57        93\n",
            "          34       0.86      0.67      0.75        90\n",
            "          35       0.95      0.79      0.87       239\n",
            "          36       0.74      0.52      0.61        66\n",
            "          37       0.50      0.03      0.06        68\n",
            "          38       0.72      0.47      0.57       100\n",
            "          39       0.40      0.10      0.16        83\n",
            "          40       0.76      0.51      0.61        83\n",
            "          41       0.84      0.33      0.47        97\n",
            "          42       0.91      0.86      0.89        73\n",
            "          43       0.62      0.20      0.30       102\n",
            "          44       0.62      0.29      0.39        52\n",
            "          45       0.17      0.05      0.08        57\n",
            "          46       0.61      0.37      0.46        60\n",
            "          47       0.90      0.77      0.83        73\n",
            "          48       0.60      0.31      0.41        87\n",
            "          49       0.93      0.74      0.83        96\n",
            "          50       0.88      0.83      0.85        64\n",
            "          51       0.55      0.29      0.38        75\n",
            "          52       0.77      0.79      0.78        84\n",
            "          53       0.92      0.66      0.77        70\n",
            "          54       1.00      0.02      0.03        62\n",
            "          55       0.67      0.47      0.55        59\n",
            "          56       0.73      0.89      0.80        62\n",
            "          57       0.88      0.87      0.88        69\n",
            "          58       0.78      0.72      0.75        54\n",
            "          59       0.50      0.11      0.18        54\n",
            "          60       0.69      0.14      0.23        66\n",
            "          61       0.78      0.67      0.72        48\n",
            "          62       0.50      0.25      0.33        57\n",
            "          63       0.71      0.10      0.17        52\n",
            "          64       0.57      0.24      0.34        54\n",
            "          65       0.00      0.00      0.00        57\n",
            "          66       0.61      0.68      0.64        34\n",
            "          67       0.00      0.00      0.00        67\n",
            "          68       0.00      0.00      0.00        36\n",
            "          69       0.50      0.35      0.41        37\n",
            "          70       0.00      0.00      0.00        49\n",
            "          71       0.00      0.00      0.00        35\n",
            "          72       0.57      0.15      0.24        53\n",
            "          73       0.40      0.12      0.19        33\n",
            "          74       0.43      0.06      0.11        49\n",
            "          75       0.73      0.22      0.34        36\n",
            "          76       0.78      0.60      0.68        35\n",
            "          77       0.56      0.12      0.20        42\n",
            "          78       0.81      0.72      0.76        53\n",
            "          79       0.00      0.00      0.00        42\n",
            "          80       0.92      0.86      0.89        64\n",
            "          81       0.25      0.06      0.09        36\n",
            "          82       0.79      0.38      0.51        58\n",
            "          83       0.65      0.31      0.42        35\n",
            "          84       0.58      0.44      0.50        41\n",
            "          85       0.92      0.76      0.83        58\n",
            "          86       0.52      0.35      0.42        40\n",
            "          87       0.33      0.09      0.14        47\n",
            "          88       0.67      0.10      0.17        40\n",
            "          89       1.00      0.02      0.05        43\n",
            "          90       0.81      0.73      0.77        41\n",
            "          91       0.71      0.50      0.59        34\n",
            "          92       0.78      0.46      0.58        54\n",
            "          93       0.70      0.49      0.58        39\n",
            "          94       1.00      0.10      0.18        30\n",
            "          95       0.60      0.13      0.21        23\n",
            "          96       0.50      0.33      0.40        36\n",
            "          97       0.83      0.74      0.79        47\n",
            "          98       0.78      0.35      0.48        40\n",
            "          99       0.88      0.65      0.75        46\n",
            "         100       0.48      0.34      0.40        29\n",
            "         101       0.85      0.49      0.62        35\n",
            "         102       0.91      0.80      0.85        51\n",
            "         103       0.62      0.14      0.23        35\n",
            "         104       1.00      0.03      0.06        31\n",
            "         105       0.68      0.61      0.64        28\n",
            "         106       0.89      0.73      0.80        45\n",
            "         107       0.00      0.00      0.00        42\n",
            "         108       0.00      0.00      0.00        26\n",
            "         109       0.50      0.04      0.07        28\n",
            "         110       0.67      0.06      0.10        36\n",
            "         111       0.74      0.72      0.73        40\n",
            "         112       0.00      0.00      0.00        30\n",
            "         113       0.33      0.06      0.11        16\n",
            "         114       1.00      0.09      0.16        23\n",
            "         115       0.00      0.00      0.00        29\n",
            "         116       0.82      0.33      0.47        27\n",
            "         117       0.86      0.81      0.83        37\n",
            "         118       0.00      0.00      0.00        31\n",
            "         119       0.56      0.35      0.43        26\n",
            "         120       0.58      0.19      0.29        36\n",
            "         121       0.69      0.29      0.41        31\n",
            "         122       0.57      0.10      0.17        40\n",
            "         123       0.00      0.00      0.00        43\n",
            "         124       0.00      0.00      0.00        29\n",
            "         125       0.00      0.00      0.00        36\n",
            "         126       0.00      0.00      0.00        28\n",
            "         127       1.00      0.06      0.11        18\n",
            "         128       0.33      0.03      0.06        31\n",
            "         129       0.00      0.00      0.00        18\n",
            "         130       0.00      0.00      0.00        35\n",
            "         131       0.62      0.20      0.30        25\n",
            "         132       0.50      0.03      0.06        30\n",
            "         133       0.65      0.71      0.68        21\n",
            "         134       0.00      0.00      0.00        41\n",
            "         135       0.67      0.06      0.11        32\n",
            "         136       1.00      0.03      0.07        29\n",
            "         137       0.33      0.30      0.32        23\n",
            "         138       0.76      0.45      0.57        29\n",
            "         139       0.82      0.58      0.68        24\n",
            "         140       0.96      0.76      0.85        33\n",
            "         141       0.78      0.76      0.77        33\n",
            "         142       0.00      0.00      0.00        18\n",
            "         143       0.69      0.58      0.63        19\n",
            "         144       0.71      0.10      0.18        49\n",
            "         145       0.43      0.18      0.26        33\n",
            "         146       0.00      0.00      0.00        34\n",
            "         147       0.75      0.64      0.69        14\n",
            "         148       0.29      0.14      0.19        14\n",
            "         149       0.82      0.82      0.82        22\n",
            "         150       0.60      0.11      0.18        28\n",
            "         151       0.39      0.29      0.33        24\n",
            "         152       0.33      0.04      0.07        27\n",
            "         153       0.61      0.42      0.50        26\n",
            "         154       0.50      0.15      0.23        20\n",
            "         155       0.88      0.60      0.71        25\n",
            "         156       0.89      0.85      0.87        20\n",
            "         157       0.75      0.41      0.53        22\n",
            "         158       0.65      0.42      0.51        26\n",
            "         159       0.00      0.00      0.00        28\n",
            "         160       0.78      0.32      0.45        22\n",
            "         161       0.00      0.00      0.00        28\n",
            "         162       0.62      0.52      0.57        25\n",
            "         163       0.00      0.00      0.00        18\n",
            "         164       0.76      0.76      0.76        25\n",
            "         165       0.00      0.00      0.00        20\n",
            "         166       0.88      0.72      0.79        53\n",
            "         167       0.76      0.46      0.57        35\n",
            "         168       0.79      0.75      0.77        20\n",
            "         169       0.50      0.12      0.19        17\n",
            "         170       0.00      0.00      0.00        23\n",
            "         171       1.00      0.08      0.15        24\n",
            "         172       0.00      0.00      0.00        21\n",
            "         173       0.53      0.60      0.56        15\n",
            "         174       1.00      0.37      0.54        19\n",
            "         175       0.58      0.58      0.58        12\n",
            "         176       0.77      0.39      0.52        44\n",
            "         177       0.50      0.18      0.27        22\n",
            "         178       0.84      0.97      0.90        32\n",
            "         179       0.00      0.00      0.00        12\n",
            "         180       1.00      0.08      0.15        12\n",
            "         181       0.00      0.00      0.00        26\n",
            "         182       0.50      0.29      0.36        14\n",
            "         183       0.71      0.43      0.54        23\n",
            "         184       0.59      0.71      0.65        14\n",
            "         185       0.33      0.06      0.10        17\n",
            "         186       0.77      0.67      0.71        30\n",
            "         187       0.67      0.08      0.14        25\n",
            "         188       0.25      0.09      0.13        11\n",
            "         189       0.00      0.00      0.00         8\n",
            "         190       0.00      0.00      0.00        23\n",
            "         191       0.90      0.96      0.93        28\n",
            "         192       0.88      0.75      0.81        28\n",
            "         193       0.00      0.00      0.00        22\n",
            "         194       0.69      0.48      0.56        23\n",
            "         195       0.87      0.67      0.75        30\n",
            "         196       0.86      0.86      0.86        22\n",
            "         197       0.00      0.00      0.00        15\n",
            "         198       1.00      0.06      0.11        18\n",
            "         199       0.00      0.00      0.00        12\n",
            "\n",
            "   micro avg       0.76      0.45      0.57     16530\n",
            "   macro avg       0.58      0.35      0.40     16530\n",
            "weighted avg       0.68      0.45      0.52     16530\n",
            " samples avg       0.56      0.46      0.48     16530\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItSxfjibWD69",
        "outputId": "85f0b437-ad37-4318-e921-d7da56ffe0a3"
      },
      "source": [
        "start = datetime.now()\n",
        "questions_with_code=0\n",
        "for i in range(min(data_size, len(questions))):\n",
        "    is_code = 0\n",
        "\n",
        "    title = df['Title'][i]\n",
        "    question = df['Body'][i]\n",
        "    tag = df['Tag'][i]\n",
        "    \n",
        "    if '<code>' in question:\n",
        "        questions_with_code+=1\n",
        "        is_code = 1\n",
        "    x = len(question)+len(title)\n",
        "\n",
        "    code = str(re.findall(r'<code>(.*?)</code>', question, flags=re.DOTALL))\n",
        "\n",
        "    question=re.sub('<code>(.*?)</code>', '', question, flags=re.MULTILINE|re.DOTALL)\n",
        "    question=removeHtmlTags(question.encode('utf-8'))\n",
        "\n",
        "    title=title.encode('utf-8')\n",
        "\n",
        "    question=str(title)+\" \"+ str(title)+\" \"+ str(title)+\" \"+str(title)+\" \"+str(title)+\" \"+str(question)\n",
        "    question=re.sub(r'[^A-Za-z]+',' ',question)\n",
        "    words=word_tokenize(str(question.lower()))\n",
        "    question=' '.join(str(stemmer.stem(j)) for j in words if j not in stop_words and (len(j)!=1 or j=='c'))\n",
        "    tup = (question,code,is_code,tag)\n",
        "    preprossed_data.append(tup)\n",
        "    questions_proccesed += 1\n",
        "    if (questions_proccesed%10000==0):\n",
        "        print(\"number of questions completed=\",questions_proccesed)\n",
        "    \n",
        "print(\"Time take to execute this cell \", datetime.now()-start)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of questions completed= 60000\n",
            "number of questions completed= 70000\n",
            "number of questions completed= 80000\n",
            "number of questions completed= 90000\n",
            "number of questions completed= 100000\n",
            "Time take to execute this cell  0:01:31.768460\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8G9dKwxXcOF"
      },
      "source": [
        "data = pd.DataFrame(preprossed_data[:data_size], columns =['question', 'code', 'is_code','tag'])\n",
        "data.drop(['is_code','code'], axis =1, inplace=True)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTyzOh1RXjf2"
      },
      "source": [
        "total_size=data.shape[0]\n",
        "train_size=int(0.80*total_size)\n",
        "\n",
        "x_train=data.head(train_size)\n",
        "x_test=data.tail(total_size - train_size)\n",
        "\n",
        "y_train = multilabel_yx[0:train_size,:]\n",
        "y_test = multilabel_yx[train_size:total_size,:]"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qB85aQEnXwYB",
        "outputId": "4314f936-091e-4e2a-98f4-735b69e44b48"
      },
      "source": [
        "start = datetime.now()\n",
        "vectorizer = TfidfVectorizer(min_df=0.00009, max_features=200000, smooth_idf=True, norm=\"l2\", \\\n",
        "                             tokenizer = lambda x: x.split(), sublinear_tf=False, ngram_range=(1,3))\n",
        "x_train_multilabel = vectorizer.fit_transform(x_train['question'])\n",
        "x_test_multilabel = vectorizer.transform(x_test['question'])\n",
        "print(\"Time taken to run this cell :\", datetime.now() - start)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time taken to run this cell : 0:00:32.955623\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtdvkO75X1Se",
        "outputId": "e3825f12-91f1-4ac8-eff4-1399a69abd7a"
      },
      "source": [
        "classifier = OneVsRestClassifier(SGDClassifier(loss='log', alpha=0.00001, penalty='l1'), n_jobs=-1)\n",
        "classifier.fit(x_train_multilabel, y_train)\n",
        "predictions = classifier.predict(x_test_multilabel)\n",
        "\n",
        "print(\"accuracy :\",metrics.accuracy_score(y_test,predictions))\n",
        "print(\"macro f1 score :\",metrics.f1_score(y_test, predictions, average = 'macro'))\n",
        "print(\"micro f1 scoore :\",metrics.f1_score(y_test, predictions, average = 'micro'))\n",
        "print(\"hamming loss :\",metrics.hamming_loss(y_test,predictions))\n",
        "print(\"Precision recall report :\\n\",metrics.classification_report(y_test, predictions))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy : 0.2849\n",
            "macro f1 score : 0.41532606834244645\n",
            "micro f1 scoore : 0.5409293298321727\n",
            "hamming loss : 0.0060315\n",
            "Precision recall report :\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.32      0.42      1116\n",
            "           1       0.80      0.57      0.66       802\n",
            "           2       0.55      0.18      0.27       540\n",
            "           3       0.84      0.63      0.72       743\n",
            "           4       0.80      0.49      0.61       484\n",
            "           5       0.73      0.51      0.60       556\n",
            "           6       0.70      0.33      0.45       449\n",
            "           7       0.89      0.75      0.81       520\n",
            "           8       0.86      0.61      0.71       406\n",
            "           9       0.91      0.66      0.77       483\n",
            "          10       0.59      0.34      0.43       287\n",
            "          11       0.55      0.27      0.36       318\n",
            "          12       0.58      0.44      0.50       186\n",
            "          13       0.85      0.67      0.75       288\n",
            "          14       0.84      0.70      0.76       200\n",
            "          15       0.68      0.26      0.37       195\n",
            "          16       0.94      0.70      0.80       214\n",
            "          17       0.78      0.57      0.66       221\n",
            "          18       0.88      0.74      0.81       163\n",
            "          19       0.61      0.27      0.38       218\n",
            "          20       0.24      0.08      0.12       131\n",
            "          21       0.70      0.52      0.60       145\n",
            "          22       0.67      0.66      0.66       145\n",
            "          23       0.46      0.25      0.32       102\n",
            "          24       0.18      0.05      0.08       115\n",
            "          25       0.83      0.40      0.54       126\n",
            "          26       0.64      0.39      0.48        95\n",
            "          27       0.92      0.74      0.82       124\n",
            "          28       0.72      0.46      0.56       127\n",
            "          29       0.97      0.78      0.87       162\n",
            "          30       0.57      0.32      0.41        79\n",
            "          31       0.50      0.17      0.26       103\n",
            "          32       0.42      0.12      0.19        81\n",
            "          33       0.62      0.52      0.56        93\n",
            "          34       0.89      0.46      0.60        90\n",
            "          35       0.97      0.69      0.81       239\n",
            "          36       0.74      0.42      0.54        66\n",
            "          37       0.40      0.09      0.14        68\n",
            "          38       0.74      0.39      0.51       100\n",
            "          39       0.38      0.23      0.29        83\n",
            "          40       0.81      0.42      0.56        83\n",
            "          41       0.72      0.27      0.39        97\n",
            "          42       0.92      0.75      0.83        73\n",
            "          43       0.46      0.18      0.26       102\n",
            "          44       0.54      0.27      0.36        52\n",
            "          45       0.12      0.07      0.09        57\n",
            "          46       0.67      0.37      0.47        60\n",
            "          47       0.92      0.66      0.77        73\n",
            "          48       0.65      0.43      0.51        87\n",
            "          49       0.93      0.52      0.67        96\n",
            "          50       0.92      0.73      0.82        64\n",
            "          51       0.61      0.29      0.40        75\n",
            "          52       0.77      0.74      0.75        84\n",
            "          53       0.93      0.60      0.73        70\n",
            "          54       0.53      0.15      0.23        62\n",
            "          55       0.72      0.56      0.63        59\n",
            "          56       0.75      0.71      0.73        62\n",
            "          57       0.92      0.80      0.85        69\n",
            "          58       0.78      0.57      0.66        54\n",
            "          59       0.36      0.07      0.12        54\n",
            "          60       0.48      0.15      0.23        66\n",
            "          61       0.71      0.52      0.60        48\n",
            "          62       0.41      0.25      0.31        57\n",
            "          63       0.82      0.17      0.29        52\n",
            "          64       0.52      0.28      0.36        54\n",
            "          65       0.33      0.09      0.14        57\n",
            "          66       0.67      0.59      0.62        34\n",
            "          67       0.40      0.09      0.15        67\n",
            "          68       0.14      0.03      0.05        36\n",
            "          69       0.54      0.41      0.46        37\n",
            "          70       0.00      0.00      0.00        49\n",
            "          71       0.00      0.00      0.00        35\n",
            "          72       0.56      0.34      0.42        53\n",
            "          73       0.24      0.12      0.16        33\n",
            "          74       0.55      0.12      0.20        49\n",
            "          75       0.53      0.28      0.36        36\n",
            "          76       0.82      0.51      0.63        35\n",
            "          77       0.40      0.10      0.15        42\n",
            "          78       0.82      0.62      0.71        53\n",
            "          79       0.53      0.19      0.28        42\n",
            "          80       0.94      0.80      0.86        64\n",
            "          81       0.40      0.17      0.24        36\n",
            "          82       0.86      0.31      0.46        58\n",
            "          83       0.55      0.34      0.42        35\n",
            "          84       0.53      0.49      0.51        41\n",
            "          85       0.93      0.69      0.79        58\n",
            "          86       0.61      0.35      0.44        40\n",
            "          87       0.42      0.21      0.28        47\n",
            "          88       0.42      0.12      0.19        40\n",
            "          89       0.41      0.21      0.28        43\n",
            "          90       0.79      0.66      0.72        41\n",
            "          91       0.65      0.38      0.48        34\n",
            "          92       0.83      0.35      0.49        54\n",
            "          93       0.65      0.56      0.60        39\n",
            "          94       0.43      0.20      0.27        30\n",
            "          95       0.33      0.17      0.23        23\n",
            "          96       0.36      0.25      0.30        36\n",
            "          97       0.79      0.66      0.72        47\n",
            "          98       0.71      0.42      0.53        40\n",
            "          99       0.89      0.54      0.68        46\n",
            "         100       0.52      0.38      0.44        29\n",
            "         101       0.83      0.43      0.57        35\n",
            "         102       0.93      0.55      0.69        51\n",
            "         103       0.50      0.26      0.34        35\n",
            "         104       0.45      0.16      0.24        31\n",
            "         105       0.65      0.46      0.54        28\n",
            "         106       0.93      0.62      0.75        45\n",
            "         107       0.00      0.00      0.00        42\n",
            "         108       0.17      0.04      0.06        26\n",
            "         109       0.29      0.07      0.11        28\n",
            "         110       0.67      0.17      0.27        36\n",
            "         111       0.87      0.65      0.74        40\n",
            "         112       0.44      0.13      0.21        30\n",
            "         113       0.38      0.19      0.25        16\n",
            "         114       1.00      0.13      0.23        23\n",
            "         115       0.23      0.10      0.14        29\n",
            "         116       0.54      0.26      0.35        27\n",
            "         117       0.90      0.70      0.79        37\n",
            "         118       0.38      0.10      0.15        31\n",
            "         119       0.50      0.35      0.41        26\n",
            "         120       0.50      0.14      0.22        36\n",
            "         121       0.53      0.32      0.40        31\n",
            "         122       0.33      0.10      0.15        40\n",
            "         123       0.17      0.02      0.04        43\n",
            "         124       0.27      0.10      0.15        29\n",
            "         125       0.18      0.06      0.09        36\n",
            "         126       0.56      0.36      0.43        28\n",
            "         127       0.60      0.17      0.26        18\n",
            "         128       0.25      0.16      0.20        31\n",
            "         129       0.00      0.00      0.00        18\n",
            "         130       0.00      0.00      0.00        35\n",
            "         131       0.62      0.32      0.42        25\n",
            "         132       0.33      0.13      0.19        30\n",
            "         133       0.67      0.67      0.67        21\n",
            "         134       0.20      0.05      0.08        41\n",
            "         135       0.55      0.19      0.28        32\n",
            "         136       0.45      0.17      0.25        29\n",
            "         137       0.50      0.43      0.47        23\n",
            "         138       0.83      0.52      0.64        29\n",
            "         139       0.92      0.50      0.65        24\n",
            "         140       0.96      0.67      0.79        33\n",
            "         141       0.80      0.73      0.76        33\n",
            "         142       0.33      0.11      0.17        18\n",
            "         143       0.71      0.53      0.61        19\n",
            "         144       1.00      0.12      0.22        49\n",
            "         145       0.44      0.21      0.29        33\n",
            "         146       0.25      0.06      0.10        34\n",
            "         147       0.75      0.43      0.55        14\n",
            "         148       0.38      0.21      0.27        14\n",
            "         149       0.88      0.64      0.74        22\n",
            "         150       0.40      0.14      0.21        28\n",
            "         151       0.30      0.25      0.27        24\n",
            "         152       0.25      0.04      0.06        27\n",
            "         153       0.73      0.42      0.54        26\n",
            "         154       0.00      0.00      0.00        20\n",
            "         155       0.87      0.52      0.65        25\n",
            "         156       0.87      0.65      0.74        20\n",
            "         157       0.60      0.41      0.49        22\n",
            "         158       0.73      0.42      0.54        26\n",
            "         159       0.36      0.14      0.21        28\n",
            "         160       0.44      0.32      0.37        22\n",
            "         161       0.33      0.07      0.12        28\n",
            "         162       0.65      0.44      0.52        25\n",
            "         163       0.50      0.11      0.18        18\n",
            "         164       0.74      0.68      0.71        25\n",
            "         165       0.33      0.10      0.15        20\n",
            "         166       0.88      0.70      0.78        53\n",
            "         167       0.73      0.23      0.35        35\n",
            "         168       0.83      0.50      0.62        20\n",
            "         169       0.50      0.41      0.45        17\n",
            "         170       0.80      0.17      0.29        23\n",
            "         171       0.67      0.25      0.36        24\n",
            "         172       0.00      0.00      0.00        21\n",
            "         173       0.67      0.53      0.59        15\n",
            "         174       1.00      0.16      0.27        19\n",
            "         175       0.50      0.50      0.50        12\n",
            "         176       0.74      0.32      0.44        44\n",
            "         177       0.38      0.14      0.20        22\n",
            "         178       0.79      0.69      0.73        32\n",
            "         179       0.00      0.00      0.00        12\n",
            "         180       0.00      0.00      0.00        12\n",
            "         181       0.00      0.00      0.00        26\n",
            "         182       0.46      0.43      0.44        14\n",
            "         183       0.75      0.52      0.62        23\n",
            "         184       0.59      0.71      0.65        14\n",
            "         185       1.00      0.06      0.11        17\n",
            "         186       0.68      0.57      0.62        30\n",
            "         187       0.67      0.32      0.43        25\n",
            "         188       0.25      0.18      0.21        11\n",
            "         189       0.00      0.00      0.00         8\n",
            "         190       0.00      0.00      0.00        23\n",
            "         191       1.00      0.86      0.92        28\n",
            "         192       0.94      0.54      0.68        28\n",
            "         193       0.18      0.09      0.12        22\n",
            "         194       0.70      0.30      0.42        23\n",
            "         195       0.84      0.53      0.65        30\n",
            "         196       0.88      0.68      0.77        22\n",
            "         197       0.00      0.00      0.00        15\n",
            "         198       0.25      0.06      0.09        18\n",
            "         199       0.00      0.00      0.00        12\n",
            "\n",
            "   micro avg       0.73      0.43      0.54     16530\n",
            "   macro avg       0.58      0.34      0.42     16530\n",
            "weighted avg       0.68      0.43      0.52     16530\n",
            " samples avg       0.51      0.43      0.44     16530\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZjgdyR4X7Fw",
        "outputId": "35087fed-2c18-4cb2-c309-737405190122"
      },
      "source": [
        "classifier = OneVsRestClassifier(SGDClassifier(loss='hinge', alpha=0.00001, penalty='l1'), n_jobs=-1)\n",
        "classifier.fit(x_train_multilabel, y_train)\n",
        "predictions = classifier.predict(x_test_multilabel)\n",
        "\n",
        "print(\"accuracy :\",metrics.accuracy_score(y_test,predictions))\n",
        "print(\"macro f1 score :\",metrics.f1_score(y_test, predictions, average = 'macro'))\n",
        "print(\"micro f1 scoore :\",metrics.f1_score(y_test, predictions, average = 'micro'))\n",
        "print(\"hamming loss :\",metrics.hamming_loss(y_test,predictions))\n",
        "print(\"Precision recall report :\\n\",metrics.classification_report(y_test, predictions))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy : 0.3139\n",
            "macro f1 score : 0.40421805482307976\n",
            "micro f1 scoore : 0.5696914424684603\n",
            "hamming loss : 0.005662\n",
            "Precision recall report :\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.59      0.34      0.43      1116\n",
            "           1       0.79      0.61      0.69       802\n",
            "           2       0.52      0.15      0.23       540\n",
            "           3       0.83      0.68      0.75       743\n",
            "           4       0.79      0.55      0.65       484\n",
            "           5       0.76      0.55      0.64       556\n",
            "           6       0.72      0.34      0.46       449\n",
            "           7       0.89      0.81      0.85       520\n",
            "           8       0.89      0.68      0.77       406\n",
            "           9       0.91      0.76      0.82       483\n",
            "          10       0.58      0.32      0.41       287\n",
            "          11       0.60      0.25      0.35       318\n",
            "          12       0.61      0.44      0.51       186\n",
            "          13       0.87      0.70      0.78       288\n",
            "          14       0.86      0.73      0.79       200\n",
            "          15       0.57      0.20      0.30       195\n",
            "          16       0.95      0.73      0.83       214\n",
            "          17       0.77      0.58      0.66       221\n",
            "          18       0.89      0.79      0.84       163\n",
            "          19       0.65      0.32      0.43       218\n",
            "          20       0.00      0.00      0.00       131\n",
            "          21       0.77      0.59      0.67       145\n",
            "          22       0.71      0.60      0.65       145\n",
            "          23       0.55      0.25      0.35       102\n",
            "          24       0.33      0.01      0.02       115\n",
            "          25       0.89      0.51      0.65       126\n",
            "          26       0.63      0.36      0.46        95\n",
            "          27       0.88      0.77      0.82       124\n",
            "          28       0.73      0.54      0.62       127\n",
            "          29       0.95      0.89      0.92       162\n",
            "          30       0.53      0.30      0.39        79\n",
            "          31       0.67      0.10      0.17       103\n",
            "          32       0.50      0.02      0.05        81\n",
            "          33       0.65      0.57      0.61        93\n",
            "          34       0.86      0.68      0.76        90\n",
            "          35       0.95      0.79      0.87       239\n",
            "          36       0.80      0.50      0.62        66\n",
            "          37       0.40      0.03      0.05        68\n",
            "          38       0.71      0.47      0.57       100\n",
            "          39       0.48      0.13      0.21        83\n",
            "          40       0.80      0.54      0.65        83\n",
            "          41       0.87      0.35      0.50        97\n",
            "          42       0.90      0.85      0.87        73\n",
            "          43       0.62      0.21      0.31       102\n",
            "          44       0.56      0.29      0.38        52\n",
            "          45       0.12      0.04      0.05        57\n",
            "          46       0.55      0.37      0.44        60\n",
            "          47       0.92      0.77      0.84        73\n",
            "          48       0.61      0.31      0.41        87\n",
            "          49       0.92      0.69      0.79        96\n",
            "          50       0.89      0.80      0.84        64\n",
            "          51       0.58      0.35      0.43        75\n",
            "          52       0.76      0.74      0.75        84\n",
            "          53       0.92      0.66      0.77        70\n",
            "          54       1.00      0.06      0.12        62\n",
            "          55       0.68      0.54      0.60        59\n",
            "          56       0.72      0.90      0.80        62\n",
            "          57       0.88      0.86      0.87        69\n",
            "          58       0.79      0.69      0.73        54\n",
            "          59       0.55      0.11      0.18        54\n",
            "          60       0.62      0.15      0.24        66\n",
            "          61       0.74      0.65      0.69        48\n",
            "          62       0.52      0.21      0.30        57\n",
            "          63       1.00      0.08      0.14        52\n",
            "          64       0.61      0.26      0.36        54\n",
            "          65       0.00      0.00      0.00        57\n",
            "          66       0.66      0.68      0.67        34\n",
            "          67       0.00      0.00      0.00        67\n",
            "          68       0.00      0.00      0.00        36\n",
            "          69       0.56      0.27      0.36        37\n",
            "          70       0.00      0.00      0.00        49\n",
            "          71       0.00      0.00      0.00        35\n",
            "          72       0.56      0.26      0.36        53\n",
            "          73       0.36      0.12      0.18        33\n",
            "          74       0.40      0.04      0.07        49\n",
            "          75       0.60      0.17      0.26        36\n",
            "          76       0.80      0.57      0.67        35\n",
            "          77       0.71      0.12      0.20        42\n",
            "          78       0.83      0.72      0.77        53\n",
            "          79       0.00      0.00      0.00        42\n",
            "          80       0.92      0.91      0.91        64\n",
            "          81       0.36      0.11      0.17        36\n",
            "          82       0.76      0.38      0.51        58\n",
            "          83       0.67      0.34      0.45        35\n",
            "          84       0.56      0.54      0.55        41\n",
            "          85       0.92      0.78      0.84        58\n",
            "          86       0.54      0.35      0.42        40\n",
            "          87       0.44      0.09      0.14        47\n",
            "          88       0.50      0.07      0.13        40\n",
            "          89       0.50      0.02      0.04        43\n",
            "          90       0.73      0.73      0.73        41\n",
            "          91       0.65      0.38      0.48        34\n",
            "          92       0.82      0.52      0.64        54\n",
            "          93       0.70      0.54      0.61        39\n",
            "          94       0.50      0.13      0.21        30\n",
            "          95       0.67      0.09      0.15        23\n",
            "          96       0.50      0.36      0.42        36\n",
            "          97       0.84      0.77      0.80        47\n",
            "          98       0.74      0.35      0.47        40\n",
            "          99       0.88      0.61      0.72        46\n",
            "         100       0.52      0.41      0.46        29\n",
            "         101       0.90      0.51      0.65        35\n",
            "         102       0.90      0.75      0.82        51\n",
            "         103       0.73      0.23      0.35        35\n",
            "         104       1.00      0.03      0.06        31\n",
            "         105       0.67      0.57      0.62        28\n",
            "         106       0.89      0.73      0.80        45\n",
            "         107       0.00      0.00      0.00        42\n",
            "         108       0.00      0.00      0.00        26\n",
            "         109       0.50      0.04      0.07        28\n",
            "         110       1.00      0.06      0.11        36\n",
            "         111       0.78      0.72      0.75        40\n",
            "         112       0.50      0.03      0.06        30\n",
            "         113       0.00      0.00      0.00        16\n",
            "         114       1.00      0.09      0.16        23\n",
            "         115       0.00      0.00      0.00        29\n",
            "         116       0.75      0.33      0.46        27\n",
            "         117       0.91      0.78      0.84        37\n",
            "         118       0.00      0.00      0.00        31\n",
            "         119       0.48      0.42      0.45        26\n",
            "         120       0.55      0.17      0.26        36\n",
            "         121       0.58      0.23      0.33        31\n",
            "         122       0.29      0.05      0.09        40\n",
            "         123       0.00      0.00      0.00        43\n",
            "         124       0.50      0.03      0.06        29\n",
            "         125       0.00      0.00      0.00        36\n",
            "         126       0.00      0.00      0.00        28\n",
            "         127       1.00      0.06      0.11        18\n",
            "         128       1.00      0.03      0.06        31\n",
            "         129       0.00      0.00      0.00        18\n",
            "         130       0.00      0.00      0.00        35\n",
            "         131       0.62      0.20      0.30        25\n",
            "         132       0.00      0.00      0.00        30\n",
            "         133       0.65      0.71      0.68        21\n",
            "         134       1.00      0.02      0.05        41\n",
            "         135       0.56      0.16      0.24        32\n",
            "         136       1.00      0.03      0.07        29\n",
            "         137       0.43      0.39      0.41        23\n",
            "         138       0.72      0.45      0.55        29\n",
            "         139       0.83      0.62      0.71        24\n",
            "         140       0.92      0.73      0.81        33\n",
            "         141       0.79      0.82      0.81        33\n",
            "         142       0.00      0.00      0.00        18\n",
            "         143       0.75      0.63      0.69        19\n",
            "         144       0.86      0.12      0.21        49\n",
            "         145       0.43      0.18      0.26        33\n",
            "         146       0.00      0.00      0.00        34\n",
            "         147       0.82      0.64      0.72        14\n",
            "         148       0.25      0.07      0.11        14\n",
            "         149       0.83      0.91      0.87        22\n",
            "         150       0.60      0.11      0.18        28\n",
            "         151       0.25      0.17      0.20        24\n",
            "         152       0.67      0.07      0.13        27\n",
            "         153       0.73      0.42      0.54        26\n",
            "         154       0.50      0.15      0.23        20\n",
            "         155       0.88      0.56      0.68        25\n",
            "         156       0.88      0.75      0.81        20\n",
            "         157       0.75      0.41      0.53        22\n",
            "         158       0.56      0.35      0.43        26\n",
            "         159       0.00      0.00      0.00        28\n",
            "         160       0.46      0.27      0.34        22\n",
            "         161       0.00      0.00      0.00        28\n",
            "         162       0.65      0.52      0.58        25\n",
            "         163       1.00      0.06      0.11        18\n",
            "         164       0.73      0.76      0.75        25\n",
            "         165       0.00      0.00      0.00        20\n",
            "         166       0.89      0.74      0.80        53\n",
            "         167       0.76      0.46      0.57        35\n",
            "         168       0.75      0.75      0.75        20\n",
            "         169       0.50      0.12      0.19        17\n",
            "         170       0.00      0.00      0.00        23\n",
            "         171       1.00      0.17      0.29        24\n",
            "         172       0.00      0.00      0.00        21\n",
            "         173       0.56      0.60      0.58        15\n",
            "         174       1.00      0.32      0.48        19\n",
            "         175       0.62      0.67      0.64        12\n",
            "         176       0.76      0.36      0.49        44\n",
            "         177       0.56      0.23      0.32        22\n",
            "         178       0.84      0.97      0.90        32\n",
            "         179       0.00      0.00      0.00        12\n",
            "         180       1.00      0.08      0.15        12\n",
            "         181       0.00      0.00      0.00        26\n",
            "         182       0.50      0.29      0.36        14\n",
            "         183       0.71      0.43      0.54        23\n",
            "         184       0.59      0.71      0.65        14\n",
            "         185       0.33      0.06      0.10        17\n",
            "         186       0.77      0.67      0.71        30\n",
            "         187       0.00      0.00      0.00        25\n",
            "         188       0.20      0.09      0.13        11\n",
            "         189       0.00      0.00      0.00         8\n",
            "         190       0.00      0.00      0.00        23\n",
            "         191       0.90      0.93      0.91        28\n",
            "         192       0.87      0.71      0.78        28\n",
            "         193       0.00      0.00      0.00        22\n",
            "         194       0.75      0.52      0.62        23\n",
            "         195       0.86      0.60      0.71        30\n",
            "         196       0.86      0.86      0.86        22\n",
            "         197       0.00      0.00      0.00        15\n",
            "         198       1.00      0.06      0.11        18\n",
            "         199       0.00      0.00      0.00        12\n",
            "\n",
            "   micro avg       0.77      0.45      0.57     16530\n",
            "   macro avg       0.59      0.35      0.40     16530\n",
            "weighted avg       0.69      0.45      0.52     16530\n",
            " samples avg       0.56      0.46      0.48     16530\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KkCFMYHYcdW"
      },
      "source": [
        "# classifier = OneVsRestClassifier(svm.SVC(kernel='linear'))\n",
        "# classifier.fit(x_train_multilabel, y_train)\n",
        "# predictions = classifier.predict(x_test_multilabel)\n",
        "\n",
        "# print(\"accuracy :\",metrics.accuracy_score(y_test,predictions))\n",
        "# print(\"macro f1 score :\",metrics.f1_score(y_test, predictions, average = 'macro'))\n",
        "# print(\"micro f1 scoore :\",metrics.f1_score(y_test, predictions, average = 'micro'))\n",
        "# print(\"hamming loss :\",metrics.hamming_loss(y_test,predictions))\n",
        "# print(\"Precision recall report :\\n\",metrics.classification_report(y_test, predictions))"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjcUNLXCYqg3"
      },
      "source": [
        ""
      ],
      "execution_count": 40,
      "outputs": []
    }
  ]
}